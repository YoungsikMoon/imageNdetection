{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vm7VUsM3hdXg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.init"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 랜덤 시드 고정\n",
        "torch.manual_seed(777)\n",
        "\n",
        "# GPU 사용 가능일 경우 랜덤 시드 고정\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "ei5C6VKbgBjZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100"
      ],
      "metadata": {
        "id": "TdbW0yFrgCaG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정\n",
        "                          train=True, # True를 지정하면 훈련 데이터로 다운로드\n",
        "                          transform=transforms.ToTensor(), # 텐서로 변환\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/', # 다운로드 경로 지정\n",
        "                         train=False, # False를 지정하면 테스트 데이터로 다운로드\n",
        "                         transform=transforms.ToTensor(), # 텐서로 변환\n",
        "                         download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAQXIe-ngGEq",
        "outputId": "6090ccc2-9660-4958-d13a-4a4e3a320622"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 36649892.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1050002.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 9782453.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 1047595.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)"
      ],
      "metadata": {
        "id": "sZC2Y8EkgLUp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # 첫번째층\n",
        "        # ImgIn shape=(?, 28, 28, 1)\n",
        "        #    Conv     -> (?, 28, 28, 32)\n",
        "        #    Pool     -> (?, 14, 14, 32)\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        # 두번째층\n",
        "        # ImgIn shape=(?, 14, 14, 32)\n",
        "        #    Conv      ->(?, 14, 14, 64)\n",
        "        #    Pool      ->(?, 7, 7, 64)\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        # 세번째층\n",
        "        # ImgIn shape=(?, 7, 7, 64)\n",
        "        #    Conv      ->(?, 7, 7, 128)\n",
        "        #    Pool      ->(?, 3, 3, 128)\n",
        "        self.layer3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        # 전결합층 7x7x64 inputs -> 10 outputs\n",
        "        self.fc = torch.nn.Linear(3 * 3 * 128, 10, bias=True)\n",
        "\n",
        "        # 전결합층 한정으로 가중치 초기화\n",
        "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.view(out.size(0), -1)   # 전결합층을 위해서 Flatten\n",
        "        # out = nn.LogSoftmax(self.fc(out), dim=1) # nn.NLLLoss() 와 매칭\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "a9KV7s_XhXM_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN 모델 정의\n",
        "model = CNN().to(device) # .to('cuda') == .cuda() / .to('cpu') == .cpu()"
      ],
      "metadata": {
        "id": "cyx6Ro9Lhyb0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss().to(device)    # 비용 함수에 소프트맥스 함수 포함되어져 있음.\n",
        "# criterion = torch.nn.NLLLoss().to(device)    # nn.LogSoftmax() 와 매칭\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "UDKDZrGbhzdy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_batch = len(data_loader)\n",
        "print('총 배치의 수 : {}'.format(total_batch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJZ7_AP6h1If",
        "outputId": "420fe6f7-b2a0-4e90-d2dc-39fe3706b699"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 배치의 수 : 600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader: # 미니 배치 단위로 꺼내온다. X는 미니 배치, Y는 레이블.\n",
        "        # image is already size of (28x28), no reshape\n",
        "        # label is not one-hot encoded\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        hypothesis = model(X) # forward 실행\n",
        "        cost = criterion(hypothesis, Y) # CrossEntropyLoss\n",
        "        cost.backward() # gradient 계산\n",
        "        optimizer.step() # weight 업데이트\n",
        "\n",
        "        avg_cost += cost / total_batch\n",
        "\n",
        "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM__AHl9h3Nc",
        "outputId": "020f0e54-ee9a-48b4-d53e-082c65b23ad2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch:    1] cost = 0.204364821\n",
            "[Epoch:    2] cost = 0.0482416786\n",
            "[Epoch:    3] cost = 0.0348232836\n",
            "[Epoch:    4] cost = 0.0263678413\n",
            "[Epoch:    5] cost = 0.0211546924\n",
            "[Epoch:    6] cost = 0.0174575374\n",
            "[Epoch:    7] cost = 0.0146635436\n",
            "[Epoch:    8] cost = 0.0111658396\n",
            "[Epoch:    9] cost = 0.0110918293\n",
            "[Epoch:   10] cost = 0.00856338535\n",
            "[Epoch:   11] cost = 0.00855270959\n",
            "[Epoch:   12] cost = 0.00623815693\n",
            "[Epoch:   13] cost = 0.00692172442\n",
            "[Epoch:   14] cost = 0.00637494912\n",
            "[Epoch:   15] cost = 0.00505425408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습을 진행하지 않을 것이므로 torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGd7ur2Ih8fX",
        "outputId": "47b72977-f81e-4996-f2cb-421c7259bdf2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9884999990463257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전체 코드"
      ],
      "metadata": {
        "id": "Oy7BnR-DlErk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # PyTorch 라이브러리를 임포트합니다.\n",
        "import torchvision.datasets as dsets # torchvision에서 제공하는 데이터셋을 사용하기 위해 임포트합니다.\n",
        "import torchvision.transforms as transforms # 이미지 변환 도구를 사용하기 위해 임포트합니다.\n",
        "import torch.nn.init # 신경망 가중치를 초기화하기 위한 함수를 임포트합니다.\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # GPU가 사용 가능한 경우 'cuda'를, 그렇지 않은 경우 'cpu'를 사용하도록 설정합니다.\n",
        "\n",
        "torch.manual_seed(777) # 재현 가능한 결과를 얻기 위해 랜덤 시드를 고정합니다.\n",
        "\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777) # GPU를 사용하는 경우에도 동일한 랜덤 시드를 사용하도록 설정합니다.\n",
        "\n",
        "learning_rate = 0.001 # 학습률을 설정합니다.\n",
        "training_epochs = 15 # 전체 데이터셋에 대한 학습 횟수를 설정합니다.\n",
        "batch_size = 100 # 미니배치의 크기를 설정합니다.\n",
        "\n",
        "# MNIST 데이터셋을 다운로드하고 불러옵니다.\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/', # 데이터셋이 저장될 위치를 지정합니다.\n",
        "                          train=True, # 학습 데이터셋을 불러옵니다.\n",
        "                          transform=transforms.ToTensor(), # 이미지를 PyTorch 텐서로 변환합니다.\n",
        "                          download=True) # 지정된 위치에 데이터셋이 없는 경우 인터넷에서 다운로드합니다.\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/', # 데이터셋이 저장될 위치를 지정합니다.\n",
        "                         train=False, # 테스트 데이터셋을 불러옵니다.\n",
        "                         transform=transforms.ToTensor(), # 이미지를 PyTorch 텐서로 변환합니다.\n",
        "                         download=True) # 지정된 위치에 데이터셋이 없는 경우 인터넷에서 다운로드합니다.\n",
        "\n",
        "# 데이터 로더를 생성합니다. 이는 학습 도중에 미니배치를 자동으로 생성해주는 역할을 합니다.\n",
        "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True)\n",
        "\n",
        "# CNN 모델을 정의합니다.\n",
        "class CNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # 첫 번째 층: 입력 이미지의 채널 수는 1, 출력 채널 수는 32, 커널 크기는 3, 스트라이드는 1, 패딩은 1입니다.\n",
        "        # ReLU 활성화 함수를 적용한 후, 커널 크기 2의 최대 풀링을 적용합니다.\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        # 두 번째 층: 입력 채널 수는 32, 출력 채널 수는 64, 커널 크기는 3, 스트라이드는 1, 패딩은 1입니다.\n",
        "        # ReLU 활성화 함수를 적용한 후, 커널 크기 2의 최대 풀링을 적용합니다.\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        # 세 번째 층: 입력 채널 수는 64, 출력 채널 수는 128, 커널 크기는 3, 스트라이드는 1, 패딩은 1입니다.\n",
        "        # ReLU 활성화 함수를 적용한 후, 커널 크기 2의 최대 풀링을 적용합니다.\n",
        "        self.layer3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        # 전결합층: 3*3*128 크기의 입력을 받아 10개의 클래스에 대한 확률을 출력합니다.\n",
        "        self.fc = torch.nn.Linear(3 * 3 * 128, 10, bias=True)\n",
        "\n",
        "        # 전결합층의 가중치를 Xavier 초기화를 사용하여 초기화합니다.\n",
        "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x) # 첫 번째 층의 연산을 수행합니다.\n",
        "        out = self.layer2(out) # 두 번째 층의 연산을 수행합니다.\n",
        "        out = self.layer3(out) # 세 번째 층의 연산을 수행합니다.\n",
        "        out = out.view(out.size(0), -1)   # 전결합층에 넣기 위해 텐서를 펼칩니다.\n",
        "        out = self.fc(out) # 전결합층의 연산을 수행합니다.\n",
        "        return out\n",
        "\n",
        "# CNN 모델을 생성하고 지정한 장치에 할당합니다.\n",
        "model = CNN().to(device)\n",
        "\n",
        "# 비용 함수와 최적화 도구를 설정합니다.\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device) # 비용 함수로 크로스 엔트로피를 사용합니다.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # 최적화 도구로 Adam을 사용합니다.\n",
        "\n",
        "total_batch = len(data_loader) # 한 에포크에 필요한 미니배치의 수를 계산합니다.\n",
        "print('총 배치의 수 : {}'.format(total_batch))\n",
        "\n",
        "# 모델을 학습합니다.\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for X, Y in data_loader: # 미니배치 단위로 데이터를 가져옵니다.\n",
        "        X = X.to(device) # 입력 이미지를 지정한 장치에 할당합니다.\n",
        "        Y = Y.to(device) # 레이블을 지정한 장치에 할당합니다.\n",
        "\n",
        "        optimizer.zero_grad() # 기울기를 0으로 초기화합니다.\n",
        "        hypothesis = model(X) # 모델에 입력을 전달하여 예측값을 계산합니다.\n",
        "        cost = criterion(hypothesis, Y) # 예측값과 레이블을 비용 함수에 전달하여 비용을 계산합니다.\n",
        "        cost.backward() # 비용 함수에 대한 기울기를 계산합니다.\n",
        "        optimizer.step() # 계산한 기울기를 사용하여 모델의 파라미터를 업데이트합니다.\n",
        "\n",
        "        avg_cost += cost / total_batch # 평균 비용을 계산합니다.\n",
        "\n",
        "    print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n",
        "\n",
        "# 모델을 평가합니다.\n",
        "with torch.no_grad(): # 기울기 계산을 비활성화합니다.\n",
        "    X_test = mnist_test.test_data.view(len(mnist_test), 1, 28, 28).float().to(device) # 테스트 이미지를 지정한 장치에 할당합니다.\n",
        "    Y_test = mnist_test.test_labels.to(device) # 테스트 레이블을 지정한 장치에 할당합니다.\n",
        "\n",
        "    prediction = model(X_test) # 모델에 테스트 이미지를 전달하여 예측값을 계산합니다.\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test # 예측값과 레이블을 비교하여 정확도를 계산합니다.\n",
        "    accuracy = correct_prediction.float().mean() # 정확도를 계산합니다.\n",
        "    print('Accuracy:', accuracy.item()) # 정확도를 출력합니다."
      ],
      "metadata": {
        "id": "7z_GdMFmlGIh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}